{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c470bd-0bd9-469c-8b3b-1c62737f1fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from etl_resources import sqlite_connection, get_api_key, get_symbol_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394fe679-8371-4306-b0cb-e3b17b1448ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_data():\n",
    "    \n",
    "    con = sqlite_connection()\n",
    "    \n",
    "    qry = '''\n",
    "select \n",
    "w.*,\n",
    "b.*,\n",
    "c.*,\n",
    "i.*,\n",
    "cq.percentchange as cpi_perc_change,\n",
    "cq.valuechange as cpi_val_change,\n",
    "cq.value as cpi_value,\n",
    "f.percentchange as effr_perc_change,\n",
    "f.valuechange as effr_val_change,\n",
    "f.value as effr_value,\n",
    "g.percentchange as gdp_perc_change,\n",
    "g.valuechange as gdp_val_change,\n",
    "g.value as gdp_value,\n",
    "r.percentchange as retail_perc_change,\n",
    "r.valuechange as retail_val_change,\n",
    "r.value as retail_value,\n",
    "u.percentchange as unemp_perc_change,\n",
    "u.valuechange as unemp_val_change,\n",
    "u.value as unemp_value\n",
    "\n",
    "\n",
    "from \n",
    "\n",
    "weekly_prices_qtr w\n",
    "\n",
    "left join balance_sheet_qtr b on b.ticker = w.ticker and b.quarter = w.quarter and b.year = w.year\n",
    "left join cash_flow_qtr c on c.ticker = w.ticker and c.quarter = w.quarter and c.year = w.year\n",
    "left join income_statement_qtr i on i.ticker = w.ticker and i.quarter = w.quarter and i.year = w.year\n",
    "left join cpi_qtr cq on cq.quarter = w.quarter and cq.year = w.year\n",
    "left join federal_funds_qtr f on f.quarter = w.quarter and f.year = w.year\n",
    "left join gdp_qtr g on g.quarter = w.quarter and g.year = w.year\n",
    "left join retail_sales_qtr r on r.quarter = w.quarter and r.year = w.year\n",
    "left join unemployment_qtr u on u.quarter = w.quarter and u.year = w.year\n",
    "\n",
    "where b.fiscaldateending is not null\n",
    "\n",
    "order by w.ticker, w.year, w.quarter\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_sql(qry, con=con)\n",
    "    \n",
    "    df = df.loc[:,~df.columns.duplicated()]\n",
    "    \n",
    "    df.to_sql(name='training', con=con, if_exists='replace')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8860883b-a037-4759-9215-83e14f1574af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_data():\n",
    "    \n",
    "    con = sqlite_connection()\n",
    "    \n",
    "    qry = '''select * from training_clean'''\n",
    "    \n",
    "    df = pd.read_sql(qry, con=con)\n",
    "    \n",
    "    profile = ProfileReport(df, title='training profile')\n",
    "    profile.to_file('../data/profiles/pre-training.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6403e7bf-e261-4e91-bca2-a1efb6745bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(df):\n",
    "    \n",
    "    non_nums = ['date','index','quarter','year','ticker','fiscaldateending','close','close_pct','close_val']\n",
    "    \n",
    "    proc_cols = [col for col in df.columns if col not in non_nums]\n",
    "\n",
    "    return proc_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cf280d-a34b-437f-907e-f0374d280599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bump_outliers(df):\n",
    "    \n",
    "    '''\n",
    "    This method iterates over the columns and removes any rows that are 3 std devs \n",
    "    outside of the mean in either direction\n",
    "    '''\n",
    "    outlier_cols = get_columns(df)\n",
    "    \n",
    "    for col in outlier_cols:\n",
    "        \n",
    "        try:\n",
    "            std_dev = df[col].std()\n",
    "            mean = df[col].mean()\n",
    "\n",
    "            upper_bound = mean + 3*std_dev \n",
    "            lower_bound = mean - 3*std_dev\n",
    "\n",
    "            df = df[df[col] > lower_bound]\n",
    "            df = df[df[col] < upper_bound]\n",
    "        \n",
    "        except:\n",
    "            print(f'Outlier logic failed on {col}')\n",
    "        \n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8673ce7b-6c4f-4984-9ca4-6030dbdbfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scaler(df):\n",
    "    \n",
    "    '''\n",
    "    This method applies the standard scaler transformation from scikit\n",
    "    '''\n",
    "    scale_cols = get_columns(df)\n",
    "    scale_cols = [col for col in scale_cols if '_pct' not in col]\n",
    "    \n",
    "    #scaler = StandardScaler()\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    for col in scale_cols:\n",
    "        try:\n",
    "            df[col] = scaler.fit_transform(df[[col]])\n",
    "        except:\n",
    "            print(f\"Scaling failed on {col}\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b6d8c8-cc03-4f87-bc8f-ef309cce91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bump_correlation(df):\n",
    "    \n",
    "    correlated_features = set()\n",
    "    correlation_matrix = df.corr()\n",
    "    \n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_features.add(colname)\n",
    "    \n",
    "    return df.drop(columns=correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44a5a559-2cb1-477b-af23-e7dabd1dba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rounding(df):\n",
    "    \n",
    "    rounding_cols = get_columns(df)\n",
    "    \n",
    "    for col in rounding_cols:\n",
    "        df[col] = pd.to_numeric(df[col],errors='coerce') \n",
    "        df[col] = df[col].round(4)\n",
    "\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb3ca547-fe70-46b5-8cf4-9fdba445e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    df = base_data()\n",
    "    df = df.fillna(0.0)\n",
    "    df = df.replace([np.inf, -np.inf], 0.0)\n",
    "    df = apply_scaler(df)\n",
    "    df = rounding(df)\n",
    "    df = bump_correlation(df)\n",
    "    #df = bump_outliers(df)\n",
    "    \n",
    "    df.to_sql(name='training_clean', con = sqlite_connection(), if_exists='replace')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d74f6dc-f41e-40a3-ba78-241592ffd2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
