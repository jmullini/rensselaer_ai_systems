{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a08abd2-a032-4586-b5c7-6f0439a3c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from etl_resources import sqlite_connection\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandasql as psql\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19eb7144-32ad-4980-9d35-37912556f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 19:31:06.425648: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-11-30 19:31:06.425714: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c766512b-bd3c-4475-9419-9a77e67f82b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_list():\n",
    "    \n",
    "    con = sqlite_connection()\n",
    "    cur = con.cursor()\n",
    "    cur.execute('''\n",
    "    select distinct w.ticker from weekly_prices_clean w\n",
    "        inner join (select ticker from weekly_prices_clean\n",
    "        group by ticker\n",
    "        having max(date) > '2021-01-01') t on t.ticker=w.ticker\n",
    "        \n",
    "    ''')\n",
    "    res = cur.fetchall()\n",
    "    res = [val[0] for val in res]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db16f7c-7cc2-45dc-a229-f70e39459049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_df(ticker):\n",
    "    \n",
    "    '''\n",
    "    This function returns the base time series dataframe (date and close)\n",
    "    '''\n",
    "    \n",
    "    con = sqlite_connection()\n",
    "        \n",
    "    df = pd.read_sql(f'''select date,close \n",
    "    from weekly_prices_clean where ticker='{ticker}' --and date>'2017-12-31' \n",
    "    group by date,close\n",
    "    order by date asc''',con=con)\n",
    "    \n",
    "    df = df.filter(['close'])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3483f05-aa3a-45e0-9f32-7f4bc3616ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_perc_err(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805b9692-22a8-4da7-92a5-df1be4c74e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_end(self, epochs, logs={}) :\n",
    "        threshold = 0.002\n",
    "        if(logs.get('loss') is not None and logs.get('loss') < threshold) :\n",
    "            print('\\nLoss dropped below {}, cancelling further training'.format(str(threshold)))\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a0d7f9-40a8-4d0c-be4f-dfa870195542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(x_train, y_train, x_test, y_test, scaler):\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    callbacks = TFCallback()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, batch_size=1, epochs=50, callbacks=[callbacks])\n",
    "    \n",
    "    # Convert the data to a numpy array\n",
    "    x_test = np.array(x_test)\n",
    "\n",
    "    # Reshape the data\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "\n",
    "    # Get the models predicted price values \n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "    # Get the root mean squared error (RMSE)\n",
    "    rmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))\n",
    "    mape = mean_abs_perc_err(y_test, predictions)\n",
    "    print('RMSE:',rmse)\n",
    "    print('MAPE:',mape)\n",
    "    \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116a494-199e-407d-8741-6c099f67fd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for MMM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 19:31:08.116100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-11-30 19:31:08.116133: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-11-30 19:31:08.116151: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jameslaptop-Lenovo-Yoga-2-Pro): /proc/driver/nvidia/version does not exist\n",
      "2021-11-30 19:31:08.116339: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-30 19:31:08.569299: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 31s 28ms/step - loss: 0.0030\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 32s 31ms/step - loss: 0.0012\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 5.03580128552323\n",
      "MAPE: 2.2554948232578567\n",
      "Building model for BKNG\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0023\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0014\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 196.59756307470087\n",
      "MAPE: 7.68166606268815\n",
      "Building model for ABT\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 33s 30ms/step - loss: 0.0023\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 32s 31ms/step - loss: 0.0010\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 7.758655694677558\n",
      "MAPE: 5.885838584861403\n",
      "Building model for ABBV\n",
      "Epoch 1/50\n",
      "379/379 [==============================] - 14s 31ms/step - loss: 0.0112\n",
      "Epoch 2/50\n",
      "379/379 [==============================] - 12s 32ms/step - loss: 0.0056\n",
      "Epoch 3/50\n",
      "379/379 [==============================] - 13s 34ms/step - loss: 0.0034\n",
      "Epoch 4/50\n",
      "379/379 [==============================] - 13s 35ms/step - loss: 0.0032\n",
      "Epoch 5/50\n",
      "379/379 [==============================] - 13s 35ms/step - loss: 0.0025\n",
      "Epoch 6/50\n",
      "379/379 [==============================] - 13s 33ms/step - loss: 0.0023\n",
      "Epoch 7/50\n",
      "379/379 [==============================] - 12s 31ms/step - loss: 0.0022\n",
      "Epoch 8/50\n",
      "379/379 [==============================] - 12s 32ms/step - loss: 0.0020\n",
      "Epoch 9/50\n",
      "379/379 [==============================] - 12s 32ms/step - loss: 0.0022\n",
      "Epoch 10/50\n",
      "379/379 [==============================] - 12s 32ms/step - loss: 0.0021\n",
      "Epoch 11/50\n",
      "379/379 [==============================] - 12s 32ms/step - loss: 0.0024\n",
      "Epoch 12/50\n",
      "379/379 [==============================] - 12s 32ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 2.934389979022893\n",
      "MAPE: 2.113946991875532\n",
      "Building model for ACN\n",
      "Epoch 1/50\n",
      "947/947 [==============================] - 34s 34ms/step - loss: 0.0012\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1fd6def1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RMSE: 12.406388657225442\n",
      "MAPE: 3.7185955047244477\n",
      "Building model for ADBE\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 40s 36ms/step - loss: 0.0013\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1fdc8b3160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "RMSE: 50.224806304511425\n",
      "MAPE: 7.485850439559546\n",
      "Building model for MO\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 40s 37ms/step - loss: 0.0047\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 0.0025\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 32s 31ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 1.9858837559979234\n",
      "MAPE: 3.6210757174181376\n",
      "Building model for AMZN\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 37s 34ms/step - loss: 0.0020\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 192.74557962443245\n",
      "MAPE: 4.956896475479457\n",
      "Building model for AXP\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 39s 36ms/step - loss: 0.0021\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 7.1011e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 39.42692360504801\n",
      "MAPE: 25.52123013500893\n",
      "Building model for AIG\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 37s 34ms/step - loss: 0.0022\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 40s 39ms/step - loss: 9.9169e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 3.92059911575723\n",
      "MAPE: 7.648030296309191\n",
      "Building model for AMGN\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 38s 34ms/step - loss: 0.0032\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 38s 36ms/step - loss: 0.0014\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 36.47785640638135\n",
      "MAPE: 15.401856722241588\n",
      "Building model for AAPL\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 41s 38ms/step - loss: 0.0064\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0035\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0030\n",
      "Epoch 4/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0022\n",
      "Epoch 5/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0023\n",
      "Epoch 6/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0028\n",
      "Epoch 7/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0023\n",
      "Epoch 8/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0020\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 5.05634002324472\n",
      "MAPE: 2.847258163531311\n",
      "Building model for T\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 42s 38ms/step - loss: 0.0030\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0014\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 0.7791300054340956\n",
      "MAPE: 2.0397796157027157\n",
      "Building model for BAC\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0037\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 1.7401173169990491\n",
      "MAPE: 3.9759403430989675\n",
      "Building model for BK\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0052\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0026\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0023\n",
      "Epoch 4/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0022\n",
      "Epoch 5/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0020\n",
      "Epoch 6/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 1.605864738641437\n",
      "MAPE: 2.669966790694698\n",
      "Building model for BRK.B\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 35s 32ms/step - loss: 0.0042\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 44.07183493115758\n",
      "MAPE: 16.894706873809742\n",
      "Building model for BIIB\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 35s 32ms/step - loss: 0.0041\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0016\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 24.617522606261865\n",
      "MAPE: 4.880710308531926\n",
      "Building model for BLK\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 34s 31ms/step - loss: 0.0011\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 135.05981297797223\n",
      "MAPE: 16.2136035251842\n",
      "Building model for BMY\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 35s 32ms/step - loss: 0.0045\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 35s 33ms/step - loss: 0.0022\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0015\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 1.7398594561041247\n",
      "MAPE: 2.173513908729156\n",
      "Building model for COF\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 37s 34ms/step - loss: 0.0016\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 7.085902484552399\n",
      "MAPE: 3.916163387796744\n",
      "Building model for CAT\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 37s 34ms/step - loss: 0.0018\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 13.191295537813229\n",
      "MAPE: 5.354798358175326\n",
      "Building model for CHTR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61847/2964832072.py:53: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure(figsize=(16,6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "528/528 [==============================] - 19s 31ms/step - loss: 0.0033\n",
      "Epoch 2/50\n",
      "528/528 [==============================] - 17s 32ms/step - loss: 0.0013\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 43.91978731716733\n",
      "MAPE: 4.622280107740823\n",
      "Building model for CVX\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 33s 31ms/step - loss: 0.0080\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0036\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0028\n",
      "Epoch 4/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0026\n",
      "Epoch 5/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0026\n",
      "Epoch 6/50\n",
      "1032/1032 [==============================] - 39s 38ms/step - loss: 0.0026\n",
      "Epoch 7/50\n",
      "1032/1032 [==============================] - 36s 34ms/step - loss: 0.0024\n",
      "Epoch 8/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0022\n",
      "Epoch 9/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0024\n",
      "Epoch 10/50\n",
      "1032/1032 [==============================] - ETA: 0s - loss: 0.002 - 36s 35ms/step - loss: 0.0024\n",
      "Epoch 11/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0025\n",
      "Epoch 12/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0022\n",
      "Epoch 13/50\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0022\n",
      "Epoch 14/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0022\n",
      "Epoch 15/50\n",
      "1032/1032 [==============================] - 37s 36ms/step - loss: 0.0021\n",
      "Epoch 16/50\n",
      "1032/1032 [==============================] - 30s 29ms/step - loss: 0.0022\n",
      "Epoch 17/50\n",
      "1032/1032 [==============================] - 30s 29ms/step - loss: 0.0022\n",
      "Epoch 18/50\n",
      "1032/1032 [==============================] - 30s 29ms/step - loss: 0.0021\n",
      "Epoch 19/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0022\n",
      "Epoch 20/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0022\n",
      "Epoch 21/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0022\n",
      "Epoch 22/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 23/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 24/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 25/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 26/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 27/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 28/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 29/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 30/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 31/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 4.100141998101349\n",
      "MAPE: 3.2000552780271474\n",
      "Building model for CSCO\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 7.1396e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 2.2816881539163396\n",
      "MAPE: 3.837694355870708\n",
      "Building model for C\n",
      "Epoch 1/50\n",
      "1025/1025 [==============================] - 34s 31ms/step - loss: 0.0054\n",
      "Epoch 2/50\n",
      "1025/1025 [==============================] - 34s 33ms/step - loss: 0.0023\n",
      "Epoch 3/50\n",
      "1025/1025 [==============================] - 33s 33ms/step - loss: 0.0018\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 3.410352431463528\n",
      "MAPE: 4.515474446485824\n",
      "Building model for CL\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 32s 29ms/step - loss: 0.0050\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0033\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0028\n",
      "Epoch 4/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 2.097577487697569\n",
      "MAPE: 2.102943519783728\n",
      "Building model for CMCSA\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 35s 32ms/step - loss: 0.0049\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0028\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0017\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 1.7553512786583247\n",
      "MAPE: 2.4654736727173834\n",
      "Building model for COP\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0059\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0028\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0025\n",
      "Epoch 4/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 5/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 6/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0022\n",
      "Epoch 7/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 2.9686939940722357\n",
      "MAPE: 4.845829314419194\n",
      "Building model for COST\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 32s 30ms/step - loss: 8.9404e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 27.906906814375375\n",
      "MAPE: 5.83222469612237\n",
      "Building model for CVS\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 34s 31ms/step - loss: 0.0044\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 3.5363083255031817\n",
      "MAPE: 3.304762180630589\n",
      "Building model for DHR\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 35s 32ms/step - loss: 0.0011\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 17.298474085470296\n",
      "MAPE: 5.288495001137808\n",
      "Building model for DUK\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 34s 32ms/step - loss: 0.0051\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 4.276977711994863\n",
      "MAPE: 3.8532003511517554\n",
      "Building model for LLY\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 34s 31ms/step - loss: 8.6815e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 28.15798658618222\n",
      "MAPE: 11.647156848222943\n",
      "Building model for EMR\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0036\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0018\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 4.390739372769563\n",
      "MAPE: 4.02567621049859\n",
      "Building model for EXC\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 34s 31ms/step - loss: 0.0048\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0023\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0017\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 1.328988749257042\n",
      "MAPE: 2.3637483858261676\n",
      "Building model for XOM\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0063\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0026\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0023\n",
      "Epoch 4/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0019\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 2.5242689047402553\n",
      "MAPE: 3.684043806246805\n",
      "Building model for FB\n",
      "Epoch 1/50\n",
      "411/411 [==============================] - 14s 30ms/step - loss: 0.0030\n",
      "Epoch 2/50\n",
      "411/411 [==============================] - 14s 33ms/step - loss: 0.0021\n",
      "Epoch 3/50\n",
      "411/411 [==============================] - 14s 33ms/step - loss: 0.0010\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 37.15365239920379\n",
      "MAPE: 9.994798267448912\n",
      "Building model for FDX\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0030\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0014\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 12.589912883588056\n",
      "MAPE: 3.8362616191811187\n",
      "Building model for F\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 9.5509e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 1.3662666587644658\n",
      "MAPE: 7.711699652699957\n",
      "Building model for GD\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 35s 32ms/step - loss: 0.0039\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 10.033309035982295\n",
      "MAPE: 5.1570592353656\n",
      "Building model for GE\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 34s 31ms/step - loss: 3.4070e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 17.60863230050792\n",
      "MAPE: 9.052131679544646\n",
      "Building model for GM\n",
      "Epoch 1/50\n",
      "485/485 [==============================] - 17s 30ms/step - loss: 0.0057\n",
      "Epoch 2/50\n",
      "485/485 [==============================] - 16s 34ms/step - loss: 0.0030\n",
      "Epoch 3/50\n",
      "485/485 [==============================] - 16s 34ms/step - loss: 0.0024\n",
      "Epoch 4/50\n",
      "485/485 [==============================] - 16s 34ms/step - loss: 0.0020\n",
      "Epoch 5/50\n",
      "485/485 [==============================] - 16s 34ms/step - loss: 0.0015\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 3.3080164361855884\n",
      "MAPE: 4.7717954899770945\n",
      "Building model for GILD\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0060\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0029\n",
      "Epoch 3/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0025\n",
      "Epoch 4/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0024\n",
      "Epoch 5/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0026\n",
      "Epoch 6/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0027\n",
      "Epoch 7/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0024\n",
      "Epoch 8/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0024\n",
      "Epoch 9/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0023\n",
      "Epoch 10/50\n",
      "1032/1032 [==============================] - 34s 32ms/step - loss: 0.0024\n",
      "Epoch 11/50\n",
      "1032/1032 [==============================] - 34s 32ms/step - loss: 0.0025\n",
      "Epoch 12/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0022\n",
      "Epoch 13/50\n",
      "1032/1032 [==============================] - 33s 32ms/step - loss: 0.0022\n",
      "Epoch 14/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 15/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 16/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 17/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0023\n",
      "Epoch 18/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 19/50\n",
      "1032/1032 [==============================] - 37s 35ms/step - loss: 0.0022\n",
      "Epoch 20/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 21/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0022\n",
      "Epoch 22/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 23/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "Epoch 24/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 25/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 26/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0021\n",
      "Epoch 27/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0020\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 2.085051492346823\n",
      "MAPE: 2.627061321483183\n",
      "Building model for GOOG\n",
      "Epoch 1/50\n",
      "319/319 [==============================] - 11s 29ms/step - loss: 0.0034\n",
      "Epoch 2/50\n",
      "319/319 [==============================] - 10s 32ms/step - loss: 0.0021\n",
      "Epoch 3/50\n",
      "319/319 [==============================] - 11s 33ms/step - loss: 0.0011\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 161.84990296901833\n",
      "MAPE: 5.207491785825559\n",
      "Building model for GOOGL\n",
      "Epoch 1/50\n",
      "795/795 [==============================] - 28s 32ms/step - loss: 0.0014\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 98.33757260846008\n",
      "MAPE: 3.3347552755815864\n",
      "Building model for HD\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 34s 31ms/step - loss: 0.0014\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 15.231468239988184\n",
      "MAPE: 4.288288590348982\n",
      "Building model for INTC\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 37s 33ms/step - loss: 8.4129e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 4.069689292544076\n",
      "MAPE: 5.852685425921968\n",
      "Building model for IBM\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0037\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0016\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 4.9579168896919175\n",
      "MAPE: 2.709593358065848\n",
      "Building model for JNJ\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0022\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0012\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 6.1461463923318895\n",
      "MAPE: 3.2427498720876096\n",
      "Building model for JPM\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 35s 32ms/step - loss: 0.0023\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 9.7861e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 9.067037087602868\n",
      "MAPE: 5.462488146683529\n",
      "Building model for KMI\n",
      "Epoch 1/50\n",
      "473/473 [==============================] - 16s 29ms/step - loss: 0.0085\n",
      "Epoch 2/50\n",
      "473/473 [==============================] - 15s 31ms/step - loss: 0.0030\n",
      "Epoch 3/50\n",
      "473/473 [==============================] - ETA: 0s - loss: 0.002 - 16s 34ms/step - loss: 0.0025\n",
      "Epoch 4/50\n",
      "473/473 [==============================] - 16s 33ms/step - loss: 0.0020\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 0.8746026913184685\n",
      "MAPE: 4.161607886434709\n",
      "Building model for LMT\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 32ms/step - loss: 0.0024\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0011\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 16.017191253291628\n",
      "MAPE: 3.37431150003381\n",
      "Building model for LOW\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 34s 31ms/step - loss: 0.0013\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 9.48822159667644\n",
      "MAPE: 4.180734078153876\n",
      "Building model for MA\n",
      "Epoch 1/50\n",
      "707/707 [==============================] - 24s 31ms/step - loss: 0.0116\n",
      "Epoch 2/50\n",
      "707/707 [==============================] - 24s 34ms/step - loss: 0.0071\n",
      "Epoch 3/50\n",
      "707/707 [==============================] - 23s 32ms/step - loss: 0.0034\n",
      "Epoch 4/50\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 0.0045\n",
      "Epoch 5/50\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 0.0031\n",
      "Epoch 6/50\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 0.0036\n",
      "Epoch 7/50\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 0.0039\n",
      "Epoch 8/50\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 0.0026\n",
      "Epoch 9/50\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 0.0026\n",
      "Epoch 10/50\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 0.0029\n",
      "Epoch 11/50\n",
      "707/707 [==============================] - 23s 33ms/step - loss: 0.0017\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 95.74450109756562\n",
      "MAPE: 26.05413308998451\n",
      "Building model for MCD\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0023\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 0.0012\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 7.203328276197128\n",
      "MAPE: 2.5789632362765844\n",
      "Building model for MDT\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0025\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 34s 33ms/step - loss: 9.6732e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 3.5820528518973203\n",
      "MAPE: 2.385399576039088\n",
      "Building model for MRK\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 0.0033\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 36s 35ms/step - loss: 0.0016\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 3.3314629586528945\n",
      "MAPE: 3.5539192320680626\n",
      "Building model for MET\n",
      "Epoch 1/50\n",
      "1011/1011 [==============================] - 35s 33ms/step - loss: 0.0053\n",
      "Epoch 2/50\n",
      "1011/1011 [==============================] - 34s 33ms/step - loss: 0.0024\n",
      "Epoch 3/50\n",
      "1011/1011 [==============================] - 34s 33ms/step - loss: 0.0018\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 2.571395524134749\n",
      "MAPE: 3.9160807445863597\n",
      "Building model for MSFT\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 33s 30ms/step - loss: 8.7340e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 28.71408875535251\n",
      "MAPE: 10.684668233603057\n",
      "Building model for MDLZ\n",
      "Epoch 1/50\n",
      "952/952 [==============================] - 32s 32ms/step - loss: 0.0041\n",
      "Epoch 2/50\n",
      "952/952 [==============================] - 31s 33ms/step - loss: 0.0020\n",
      "Epoch 3/50\n",
      "952/952 [==============================] - 32s 33ms/step - loss: 0.0015\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 1.6822908536606065\n",
      "MAPE: 2.3234119477367465\n",
      "Building model for MS\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 34s 32ms/step - loss: 0.0015\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 6.0324526686356625\n",
      "MAPE: 6.699489935252828\n",
      "Building model for NEE\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 38s 35ms/step - loss: 0.0023\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 9.0417e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 50.5226150870802\n",
      "MAPE: 25.81492623438914\n",
      "Building model for NKE\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 37s 34ms/step - loss: 0.0036\n",
      "Epoch 2/50\n",
      "1032/1032 [==============================] - 35s 34ms/step - loss: 0.0016\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 10.529966897000937\n",
      "MAPE: 5.965640617953139\n",
      "Building model for NVDA\n",
      "Epoch 1/50\n",
      "1032/1032 [==============================] - 36s 33ms/step - loss: 7.2763e-04\n",
      "\n",
      "Loss dropped below 0.002, cancelling further training\n",
      "RMSE: 170.16404625043572\n",
      "MAPE: 35.76607804862593\n",
      "Building model for OXY\n",
      "Epoch 1/50\n",
      " 395/1032 [==========>...................] - ETA: 22s - loss: 0.0109"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # following https://www.kaggle.com/faressayah/stock-market-analysis-prediction-using-lstm\n",
    "    tickers = ticker_list()\n",
    "    #metrics_list = list()\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        \n",
    "        print(f\"Building model for {ticker}\")\n",
    "        \n",
    "        #metrics = dict()\n",
    "        \n",
    "        # Build the dataset and split\n",
    "        df = base_df(ticker)\n",
    "        dataset = df.values\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        scaled_data = scaler.fit_transform(dataset)\n",
    "        training_data_len = int(np.ceil( len(dataset) * .95 ))\n",
    "        train_data = scaled_data[0:int(training_data_len), :]\n",
    "    \n",
    "        # Split the data into x_train and y_train data sets\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        #print([i for i in range(100, len(train_data))])\n",
    "        for i in range(60, len(train_data)):\n",
    "            x_train.append(train_data[i-60:i, 0])\n",
    "            y_train.append(train_data[i, 0])\n",
    "       \n",
    "        x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "        x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "        test_data = scaled_data[training_data_len - 60: , :]\n",
    "        \n",
    "        # Create the data sets x_test and y_test\n",
    "        x_test = []\n",
    "        y_test = dataset[training_data_len:, :]\n",
    "        for i in range(60, len(test_data)):\n",
    "            x_test.append(test_data[i-60:i, 0])\n",
    "    \n",
    "        # Convert the data to a numpy array\n",
    "        x_test = np.array(x_test)\n",
    "\n",
    "        # Reshape the data\n",
    "        x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "        \n",
    "        # Train & Test LSTM\n",
    "        predictions = build_lstm(x_train, y_train, x_test, y_test, scaler)\n",
    "        \n",
    "        train = df[:training_data_len]\n",
    "        valid = df[training_data_len:]\n",
    "        valid['Predictions'] = predictions\n",
    "        # Visualize the data\n",
    "        plt.figure(figsize=(16,6))\n",
    "        plt.title('Model')\n",
    "        plt.xlabel('Date', fontsize=18)\n",
    "        plt.ylabel('Close Price USD ($)', fontsize=18)\n",
    "        plt.plot(train['close'])\n",
    "        plt.plot(valid[['close', 'Predictions']])\n",
    "        plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "        plt.savefig(f'../data/visualization/lstm/{ticker}.png', facecolor='white', transparent=False)\n",
    "        plt.close('all')\n",
    "        \n",
    "\n",
    "main()      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
